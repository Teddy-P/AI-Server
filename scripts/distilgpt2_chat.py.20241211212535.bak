import torch
import json
import os
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load DistilGPT-2 model and tokenizer
model_name = "distilgpt2"

try:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    print("DistilGPT-2 loaded successfully!")
except Exception as e:
    print(f"Error loading DistilGPT-2: {e}")
    exit()

# Set device (GPU if available, else CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Initialize chat history
chat_history_ids = None
max_tokens = 1024  # Max token limit for DistilGPT-2

# Global variables
# Directory where chat logs will be saved
LOG_DIR = os.path.expanduser("~/my_ai_project/chat_logs")
DATASET_DIR = os.path.expanduser("~/my_ai_project/dataset")

# Function definitions
def save_chat_log(conversation):
    """
    Saves the conversation to a JSON file in the log directory.

    Args:
        conversation (list or dict): The conversation to save.
    """
    # Ensure the log directory exists
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

    # Create a unique timestamped filename for the log
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(LOG_DIR, f"chat_{timestamp}.json")

    # Save the conversation to the JSON file
    with open(log_file, "w") as file:
        json.dump(conversation, file, indent=4)

    print(f"Chat log saved: {log_file}")

def convert_logs_to_dataset():
    """
    Converts chat logs into a dataset for fine-tuning.
    """
    # Ensure the dataset directory exists
    if not os.path.exists(DATASET_DIR):
        os.makedirs(DATASET_DIR)

    # Read all chat logs
    dataset = []
    for log_file in os.listdir(LOG_DIR):
        if log_file.endswith(".json"):
            log_path = os.path.join(LOG_DIR, log_file)
            with open(log_path, "r") as file:
                try:
                    conversation = json.load(file)
                    for exchange in conversation:
                        user_input = exchange.get("user", "")
                        ai_response = exchange.get("ai", "")
                        if user_input and ai_response:
                            dataset.append({
                                "prompt": f"{user_input}\n",  # Ensure newline after prompt
                                "completion": f" {ai_response}\n"  # Ensure newline after completion
                            })
                except json.JSONDecodeError as e:
                    print(f"Error reading {log_path}: {e}")

    # Save the dataset as a JSONL file
    dataset_file = os.path.join(DATASET_DIR, "chat_dataset.jsonl")
    with open(dataset_file, "w") as file:
        for entry in dataset:
            json.dump(entry, file)
            file.write("\n")

    print(f"Dataset created: {dataset_file}")

# Example usage (optional)
if __name__ == "__main__":
    # Example conversation
    sample_conversation = [
        {"user": "Hello!", "ai": "Hi there! How can I help you?"},
        {"user": "Can you save this conversation?", "ai": "Sure, I can do that."}
    ]

    # Save the conversation
    save_chat_log(sample_conversation)

    # Convert logs to dataset
    convert_logs_to_dataset()

# Predefined responses for simple greetings
greetings = ["hello", "hi", "greetings", "hey", "morning", "afternoon", "evening"]

print("Welcome to DistilGPT-2 Chat! Type 'exit' to quit.")

while True:
    user_input = input("You: ").strip().lower()
    if user_input == "exit":
        print("Goodbye!")
        break

    try:
        # Check if the input matches a greeting
        if any(greeting in user_input for greeting in greetings):
            print("AI: Hello! How can I assist you today?")
            continue

        # Adjust the prompt for concise and controlled responses
        prompt = f"The user said: {user_input}. Provide a brief, relevant, and friendly response."

        # Tokenize user input and add eos_token to the end of input
        new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(device)

        # Append chat history for context
        if chat_history_ids is not None:
            bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)

            # Ensure attention_mask matches bot_input_ids length
            attention_mask = torch.cat([torch.ones(chat_history_ids.shape, device=device),
                                         torch.ones(new_user_input_ids.shape, device=device)], dim=-1)

            # Keep only the last `max_history_length` exchanges in the chat history
            max_history_length = max_tokens - 150  # Reserve space for new tokens
            if bot_input_ids.shape[-1] > max_history_length:
                bot_input_ids = bot_input_ids[:, -max_history_length:]
                attention_mask = attention_mask[:, -max_history_length:]
        else:
            bot_input_ids = new_user_input_ids
            attention_mask = torch.ones(bot_input_ids.shape, device=device)  # Initialize attention_mask here for first input

        # Generate a response from the model
        chat_history_ids = model.generate(
            bot_input_ids,
            attention_mask=attention_mask,  # Pass the attention mask here
            max_new_tokens=150,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=1.0,  # Range 0.0 to 1.0 Lower temperature for more predictable output
            top_p=1.0,        # Range 0.0 to 1.0 Use top_p sampling for more controlled randomness
            top_k=50          # Range 1 to 50 Lower values ensure the model chooses from a smaller, more controlled set of options, leading to more predictable outputs.
        )

        # Decode the response and print it
        response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
        print("AI: " + response)

    except Exception as e:
        # Fallback response in case of errors
        print("AI: I'm sorry, I couldn't process that. Could you try rephrasing?")
        print(f"Debugging info: {str(e)}")
